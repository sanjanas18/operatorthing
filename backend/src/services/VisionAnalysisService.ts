// import Anthropic from '@anthropic-ai/sdk';
// import fs from 'fs';
// import path from 'path';

// interface FrameAnalysis {
//   timestamp: string;
//   hazards: string[];
//   injuries: string[];
//   environmentAssessment: string;
//   urgencyLevel: 'low' | 'medium' | 'high' | 'critical';
//   recommendations: string[];
//   rawAnalysis: string;
// }

// class VisionAnalysisService {
//   private client: Anthropic;
//   private imageStoragePath: string;

//   constructor() {
//     const apiKey = process.env.ANTHROPIC_API_KEY;
    
//     if (!apiKey) {
//       throw new Error('‚ùå Missing ANTHROPIC_API_KEY in .env file');
//     }

//     this.client = new Anthropic({
//       apiKey: apiKey,
//     });

//     // Create directory to store images
//     this.imageStoragePath = path.join(process.cwd(), 'stored_frames');
//     if (!fs.existsSync(this.imageStoragePath)) {
//       fs.mkdirSync(this.imageStoragePath, { recursive: true });
//     }
    
//     console.log('‚úÖ Claude Vision client initialized');
//   }

//   async analyzeEmergencyFrame(
//     frameBase64: string,
//     callId: string,
//     emergencyType: string,
//     recentTranscript?: string
//   ): Promise<{ analysis: FrameAnalysis; imagePath: string }> {
//     try {
//       console.log('üîç Analyzing video frame with Claude Vision...');

//       // Save image to disk
//       const imagePath = await this.saveFrame(frameBase64, callId);

//       // Remove data URL prefix if present
//       const imageData = frameBase64.replace(/^data:image\/[a-z]+;base64,/, '');

//       const prompt = `You are an emergency response AI analyzing a live video feed from a 911 call.

// EMERGENCY TYPE: ${emergencyType}
// ${recentTranscript ? `RECENT CONVERSATION:\n${recentTranscript}` : ''}

// Analyze this video frame and provide a structured emergency assessment:

// 1. VISIBLE HAZARDS: List any immediate dangers you can see (fire, smoke, weapons, broken glass, flooding, chemical spills, structural damage, electrical hazards, etc.)

// 2. INJURIES/MEDICAL: Describe any visible injuries, blood, unconscious persons, or signs of medical distress (difficulty breathing, holding body parts, facial expressions of pain, etc.)

// 3. ENVIRONMENT: Assess the location type (indoor/outdoor, residential/commercial/vehicle), safety for first responders, accessibility (stairs, narrow hallways, etc.), lighting conditions, weather if visible

// 4. URGENCY LEVEL: Rate as low/medium/high/critical based on immediate threat to life and need for rapid response

// 5. RECOMMENDATIONS: Specific actionable steps for the operator or incoming emergency units (equipment needed, number of units, special precautions, etc.)

// Be extremely detailed and thorough. Lives depend on catching every detail. Focus on actionable intelligence.`;

//       const response = await this.client.messages.create({
//         model: 'claude-3-5-sonnet-20241022',
//         max_tokens: 1024,
//         messages: [
//           {
//             role: 'user',
//             content: [
//               {
//                 type: 'image',
//                 source: {
//                   type: 'base64',
//                   media_type: 'image/jpeg',
//                   data: imageData,
//                 },
//               },
//               {
//                 type: 'text',
//                 text: prompt,
//               },
//             ],
//           },
//         ],
//         temperature: 0.3,
//       });

//     //   const analysisText = response.content[0].type === 'text' 
//     //     ? response.content[0].text 
//     //     : 'Unable to analyze frame';
//     const analysisText = (response.content && response.content[0] && response.content[0].type === 'text')
//   ? response.content[0].text 
//   : 'Unable to analyze frame';

//       // Parse the response into structured data
//       const analysis = this.parseAnalysis(analysisText);
      
//       console.log('‚úÖ Frame analysis complete:', analysis.urgencyLevel, '-', analysis.hazards.length, 'hazards detected');
      
//       return { analysis, imagePath };
//     } catch (error: any) {
//       console.error('‚ùå Vision analysis error:', error.message);
//       throw new Error('Failed to analyze frame: ' + error.message);
//     }
//   }

//   private async saveFrame(frameBase64: string, callId: string): Promise<string> {
//     // Remove data URL prefix if present
//     const base64Data = frameBase64.replace(/^data:image\/[a-z]+;base64,/, '');
    
//     // Create filename
//     const timestamp = Date.now();
//     const filename = `${callId}_${timestamp}.jpg`;
//     const filepath = path.join(this.imageStoragePath, filename);

//     // Save to disk
//     fs.writeFileSync(filepath, base64Data, 'base64');
//     console.log('üíæ Saved frame:', filename);

//     return filepath;
//   }

//   private parseAnalysis(text: string): FrameAnalysis {
//     // Extract urgency level
//     const urgencyMatch = text.toLowerCase().match(/urgency.*?(low|medium|high|critical)/);
//     const urgencyLevel = (urgencyMatch?.[1] || 'medium') as FrameAnalysis['urgencyLevel'];

//     return {
//       timestamp: new Date().toISOString(),
//       hazards: this.extractList(text, 'HAZARD|DANGER|VISIBLE HAZARDS'),
//       injuries: this.extractList(text, 'INJUR|MEDICAL|BLOOD|UNCONSCIOUS|DISTRESS'),
//       environmentAssessment: this.extractSection(text, 'ENVIRONMENT') || 'Assessment in progress',
//       urgencyLevel,
//       recommendations: this.extractList(text, 'RECOMMENDATION'),
//       rawAnalysis: text,
//     };
//   }

//  private extractList(text: string, keyword: string): string[] {
//   const regex = new RegExp(`${keyword}[^:]*:([^\\n]*(?:\\n[-‚Ä¢*]?\\s*[^\\n]*)*)`, 'i');
//   const match = text.match(regex);
//   if (!match || !match[1]) return [];  // ADD !match[1] CHECK

//   return match[1]
//     .split(/\n[-‚Ä¢*]|\n\d+\./)
//     .map(item => item.trim())
//     .filter(item => {
//       if (item.length === 0) return false;
//       if (item.match(/^(none|n\/a|not visible|no visible)/i)) return false;
//       if (item.length < 5) return false;
//       return true;
//     });
// }

// private extractSection(text: string, keyword: string): string | null {
//   const regex = new RegExp(`${keyword}[^:]*:([^\\n]+(?:\\n(?!\\d+\\.|[A-Z]+:)[^\\n]+)*)`, 'i');
//   const match = text.match(regex);
//   return (match && match[1]) ? match[1].trim() : null;  // ADD match[1] CHECK
// }


//   getStoredFrames(callId: string): string[] {
//     const files = fs.readdirSync(this.imageStoragePath);
//     return files
//       .filter(f => f.startsWith(callId))
//       .map(f => path.join(this.imageStoragePath, f));
//   }

//   deleteCallFrames(callId: string): void {
//     const frames = this.getStoredFrames(callId);
//     frames.forEach(filepath => {
//       try {
//         fs.unlinkSync(filepath);
//       } catch (error) {
//         console.error('Error deleting frame:', error);
//       }
//     });
//     console.log(`üóëÔ∏è Deleted ${frames.length} frames for call ${callId}`);
//   }
// }

// export default new VisionAnalysisService();


import Anthropic from '@anthropic-ai/sdk';
import fs from 'fs';
import path from 'path';

interface FrameAnalysis {
  timestamp: string;
  hazards: string[];
  injuries: string[];
  environmentAssessment: string;
  urgencyLevel: 'low' | 'medium' | 'high' | 'critical';
  recommendations: string[];
  rawAnalysis: string;
}

class VisionAnalysisService {
  private client: Anthropic | null = null;
  private imageStoragePath: string;

  constructor() {
    // Create directory to store images (this is safe to do immediately)
    this.imageStoragePath = path.join(process.cwd(), 'stored_frames');
    if (!fs.existsSync(this.imageStoragePath)) {
      fs.mkdirSync(this.imageStoragePath, { recursive: true });
    }
  }

  private getClient(): Anthropic {
    if (!this.client) {
      const apiKey = process.env.ANTHROPIC_API_KEY;
      
      if (!apiKey) {
        throw new Error('‚ùå Missing ANTHROPIC_API_KEY in .env file');
      }

      this.client = new Anthropic({
        apiKey: apiKey,
      });
      
      console.log('‚úÖ Claude Vision client initialized');
    }
    
    return this.client;
  }

  async analyzeEmergencyFrame(
    frameBase64: string,
    callId: string,
    emergencyType: string,
    recentTranscript?: string
  ): Promise<{ analysis: FrameAnalysis; imagePath: string }> {
    try {
      console.log('üîç Analyzing video frame with Claude Vision...');

      // Get client (lazy initialization)
      const client = this.getClient();

      // Save image to disk
      const imagePath = await this.saveFrame(frameBase64, callId);

      // Remove data URL prefix if present
      const imageData = frameBase64.replace(/^data:image\/[a-z]+;base64,/, '');

      const prompt = `You are an emergency response AI analyzing a live video feed from a 911 call.

EMERGENCY TYPE: ${emergencyType}
${recentTranscript ? `RECENT CONVERSATION:\n${recentTranscript}` : ''}

Analyze this video frame and provide a structured emergency assessment:

1. VISIBLE HAZARDS: List any immediate dangers you can see (fire, smoke, weapons, broken glass, flooding, chemical spills, structural damage, electrical hazards, etc.)

2. INJURIES/MEDICAL: Describe any visible injuries, blood, unconscious persons, or signs of medical distress (difficulty breathing, holding body parts, facial expressions of pain, etc.)

3. ENVIRONMENT: Assess the location type (indoor/outdoor, residential/commercial/vehicle), safety for first responders, accessibility (stairs, narrow hallways, etc.), lighting conditions, weather if visible

4. URGENCY LEVEL: Rate as low/medium/high/critical based on immediate threat to life and need for rapid response

5. RECOMMENDATIONS: Specific actionable steps for the operator or incoming emergency units (equipment needed, number of units, special precautions, etc.)

6. Be extremely detailed and thorough!!! Note down EVERYTHING you see even if there is no emergency.

Be extremely detailed and thorough. Lives depend on catching every detail. Focus on actionable intelligence.`;

      const response = await client.messages.create({
        model: 'claude-haiku-4-5-20251001',
        max_tokens: 1024,
        messages: [
          {
            role: 'user',
            content: [
              {
                type: 'image',
                source: {
                  type: 'base64',
                  media_type: 'image/jpeg',
                  data: imageData,
                },
              },
              {
                type: 'text',
                text: prompt,
              },
            ],
          },
        ],
        temperature: 0.3,
      });

    //   let analysisText = 'Unable to analyze frame';
    //   if (response.content && response.content.length > 0) {
    //     const firstContent = response.content[0];
    //     if (firstContent.type === 'text') {
    //       analysisText = firstContent.text;
    //     }
    //   }

    let analysisText = 'Unable to analyze frame';
if (response.content && response.content.length > 0) {
  const firstContent = response.content[0];
  if (firstContent && firstContent.type === 'text') {
    analysisText = firstContent.text;
  }
}

      // Parse the response into structured data
      const analysis = this.parseAnalysis(analysisText);
      
      console.log('‚úÖ Frame analysis complete:', analysis.urgencyLevel, '-', analysis.hazards.length, 'hazards detected');
      
      return { analysis, imagePath };
    } catch (error: any) {
      console.error('‚ùå Vision analysis error:', error.message);
      throw new Error('Failed to analyze frame: ' + error.message);
    }
  }

  private async saveFrame(frameBase64: string, callId: string): Promise<string> {
    // Remove data URL prefix if present
    const base64Data = frameBase64.replace(/^data:image\/[a-z]+;base64,/, '');
    
    // Create filename
    const timestamp = Date.now();
    const filename = `${callId}_${timestamp}.jpg`;
    const filepath = path.join(this.imageStoragePath, filename);

    // Save to disk
    fs.writeFileSync(filepath, base64Data, 'base64');
    console.log('üíæ Saved frame:', filename);

    return filepath;
  }

  private parseAnalysis(text: string): FrameAnalysis {
    // Extract urgency level
    const urgencyMatch = text.toLowerCase().match(/urgency.*?(low|medium|high|critical)/);
    const urgencyLevel = (urgencyMatch?.[1] || 'medium') as FrameAnalysis['urgencyLevel'];

    return {
      timestamp: new Date().toISOString(),
      hazards: this.extractList(text, 'HAZARD|DANGER|VISIBLE HAZARDS'),
      injuries: this.extractList(text, 'INJUR|MEDICAL|BLOOD|UNCONSCIOUS|DISTRESS'),
      environmentAssessment: this.extractSection(text, 'ENVIRONMENT') || 'Assessment in progress',
      urgencyLevel,
      recommendations: this.extractList(text, 'RECOMMENDATION'),
      rawAnalysis: text,
    };
  }

  private extractList(text: string, keyword: string): string[] {
    const regex = new RegExp(`${keyword}[^:]*:([^\\n]*(?:\\n[-‚Ä¢*]?\\s*[^\\n]*)*)`, 'i');
    const match = text.match(regex);
    if (!match || !match[1]) return [];

    return match[1]
      .split(/\n[-‚Ä¢*]|\n\d+\./)
      .map(item => item.trim())
      .filter(item => {
        if (item.length === 0) return false;
        if (item.match(/^(none|n\/a|not visible|no visible)/i)) return false;
        if (item.length < 5) return false;
        return true;
      });
  }

  private extractSection(text: string, keyword: string): string | null {
    const regex = new RegExp(`${keyword}[^:]*:([^\\n]+(?:\\n(?!\\d+\\.|[A-Z]+:)[^\\n]+)*)`, 'i');
    const match = text.match(regex);
    return (match && match[1]) ? match[1].trim() : null;
  }

  getStoredFrames(callId: string): string[] {
    const files = fs.readdirSync(this.imageStoragePath);
    return files
      .filter(f => f.startsWith(callId))
      .map(f => path.join(this.imageStoragePath, f));
  }

  deleteCallFrames(callId: string): void {
    const frames = this.getStoredFrames(callId);
    frames.forEach(filepath => {
      try {
        fs.unlinkSync(filepath);
      } catch (error) {
        console.error('Error deleting frame:', error);
      }
    });
    console.log(`üóëÔ∏è Deleted ${frames.length} frames for call ${callId}`);
  }
}

export default new VisionAnalysisService();